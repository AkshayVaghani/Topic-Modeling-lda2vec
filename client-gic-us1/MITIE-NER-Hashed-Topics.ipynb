{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors for documents using MITIE NER and feature hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook, load_workbook\n",
    "import json\n",
    "\n",
    "wb = load_workbook(filename = 'GoogleNews_18July.xlsx')\n",
    "sheet_list1 = wb['DocumentOrder']\n",
    "names=[]\n",
    "for i in range(1,173):\n",
    "    s = 'A' + str(i)\n",
    "    names.append(sheet_list1[s].value.encode(encoding='ascii',errors='ignore'))\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare document content, clean punctuations and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malaviyac/lda2vec/env/lib/python2.7/site-packages/ipykernel/__main__.py:17: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "names = []\n",
    "doc_content = []\n",
    "\n",
    "paths = [\"/home/malaviyac/lda2vec/testdata/NewSet/\", \"/home/malaviyac/lda2vec/testdata/FinanceSet/\",\n",
    "        \"/home/malaviyac/lda2vec/testdata/Education/\",\"/home/malaviyac/lda2vec/testdata/SEER/\", \n",
    "         \"/home/malaviyac/lda2vec/testdata/Duplicates/\"]\n",
    "\n",
    "bad = [\"“\", \"”\",\"'\",\"/\",'\\\\','\"',':','^','-','<','>',\"&\",'$','#','=','*','^L']\n",
    "tokens = []\n",
    "\n",
    "def clean(line):\n",
    "    line  = ''.join(w for w in line if w not in bad)\n",
    "    line  = ' '.join(w for w in line.split() if w!='^L')\n",
    "    return line\n",
    "    \n",
    "\n",
    "for path in paths:\n",
    "    if paths.index(path)<2:\n",
    "        src_files = sorted(os.listdir(path),key = lambda name : (int)(name[7:name.find(\".\")]))\n",
    "    else:\n",
    "        src_files = sorted(os.listdir(path))\n",
    "        \n",
    "    for filename in src_files:\n",
    "        with open(os.path.join(path, filename), 'r') as filedata:\n",
    "            tokens = TreebankWordTokenizer().tokenize(filedata.read())\n",
    "            doc_content.append(\"\".join(clean(\" \".join(tokens).replace(\"\\n\",\" \").decode(encoding='ascii',errors='ignore'))))\n",
    "            names.append(filename[:-4])\n",
    "\n",
    "# print doc_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature hashed vectors for top 5 entities in a document obtained from MITIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import operator\n",
    "\n",
    "# doc_content = [\"Frank Cuban is my name. What 'Frank Cuban' is this witchcraft David? Cheetah is my favorite animal. Hello, how are you! I have been thinking.\",\"Mark is a great industrialist. Apple is a bad company.\"]\n",
    "payload = {}\n",
    "entity_indices = []\n",
    "doc_matrix = []\n",
    "j=0\n",
    "d={}\n",
    "for doc in doc_content:\n",
    "    ent = []\n",
    "    \n",
    "    doc_vec = [0]*100\n",
    "    entitylist = {}\n",
    "#     print(j)\n",
    "    payload = \"\"\"{\"sentences\":[\"\"\" + '\"' + doc + '\"' + \"]}\"\n",
    "#     print(payload)\n",
    "    r = requests.post('http://fastner.sage:3020/entities/mitie', data = payload)\n",
    "    entities = r.json()['entities'][0]\n",
    "    entity_indices = [entities[i]['r'] for i in range(len(entities))]\n",
    "#     print entity_indices\n",
    "\n",
    "    for index in entity_indices:\n",
    "        entity = \" \".join(doc.split()[index[0]:index[1]])\n",
    "        if entity in entitylist.keys():\n",
    "            entitylist[entity] += 1\n",
    "        else:\n",
    "            entitylist[entity] = 1\n",
    "\n",
    "#     print entitylist.items()\n",
    "    sortedEntityList = sorted(entitylist.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # Feature hashed vector hot for top 5 entities by frequency of occurrence in each document\n",
    "    for entity in sortedEntityList[:5]:  \n",
    "        ent.append(str(entity[0]))\n",
    "        m = hashlib.md5(str(entity[0]))\n",
    "        hashed_entity = int(m.hexdigest(),16)%100\n",
    "        doc_vec[hashed_entity] = doc_vec[hashed_entity]+1\n",
    "    \n",
    "#     print(ent)\n",
    "#     print(len(entitylist))\n",
    "    d[names[j]]=ent\n",
    "    doc_matrix.append(doc_vec)\n",
    "    j=j+1\n",
    "    \n",
    "print( json.dumps( dict(entities=d) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('hashed_docs.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(vec for vec in doc_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Treebank Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# tokens = []\n",
    "# tokens = TreebankWordTokenizer().tokenize(\"Frank Cuban is my name. What 'Frank Cuban' is this witchcraft David? Cheetah is my favorite animal. David, how are you! I have been thinking.\")\n",
    "# print tokens\n",
    "# print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with tfidf vectors for the document, didn't finish the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.95,stop_words='english')\n",
    "tfidf.fit_transform(doc_content)\n",
    "tfidf_doc2topic = tfidf.transform(doc_content, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (tfidf_doc2topic.shape)\n",
    "import numpy as np\n",
    "tfidf_matrix = np.zeros((tfidf_doc2topic.shape[0],tfidf_doc2topic.shape[1]))\n",
    "tfidf_matrix = tfidf_doc2topic\n",
    "print (tfidf_matrix[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "orig = [[]]\n",
    "with open('tfidf_docs.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range((tfidf_matrix).shape[0]):\n",
    "        orig.append(tfidf_matrix[i].toarray())\n",
    "    writer.writerows(orig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexis Nexis Work beyond this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import operator\n",
    "from json import dumps\n",
    "from collections import OrderedDict\n",
    "from xml.etree.ElementTree import fromstring\n",
    "from xmljson import BadgerFish\n",
    "bf = BadgerFish(dict_type=OrderedDict)\n",
    "\n",
    "api_key = '0fc921fa3a1f4653988216cdf9483534'\n",
    "headers = {'Authorization': 'Basic SGFuZHNoYWtlczpzb3VyY2VzNDE4Mg==',\n",
    "           'content-type':'application/json',\n",
    "           'Accept':'application/json'}\n",
    "#            'WWW-Authenticate': 'Basic realm=”Rules API”'}\n",
    "\n",
    "payload = {'key': api_key}\n",
    "data = [{\"name\": \"query_us_uk\", \"query\": \"sourceCountryCode:us OR sourceCountryCode:can\", \"active\": \"true\"},\n",
    "{\"name\": \"query_gr\",\"query\": \"sourceCountryCode:can\",\"active\": \"false\"}]\n",
    "rule = requests.post(\"https://portal.moreover.com/portal-rest/v1/rules/save\",  json=data, params = payload, headers = headers)\n",
    "print rule.text\n",
    "\n",
    "rules_list = requests.get(\"https://portal.moreover.com/portal-rest/v1/rules/all\", params=payload, headers = headers)\n",
    "json_response = rules_list.json()\n",
    "\n",
    "for query in json_response[\"queries\"]:    \n",
    "    sequenceid = query[\"id\"]\n",
    "    payload = {'key': api_key, 'sequenceid': sequenceid}\n",
    "    article = requests.get(\"http://metabase.moreover.com/api/v10/articles\", params = payload, headers = headers)\n",
    "    json_article = dumps(bf.data(fromstring(article.text)))\n",
    "    print json_article\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
